{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T16:00:18.949742Z","iopub.status.busy":"2024-05-30T16:00:18.948741Z","iopub.status.idle":"2024-05-30T16:00:33.624135Z","shell.execute_reply":"2024-05-30T16:00:33.623141Z","shell.execute_reply.started":"2024-05-30T16:00:18.949692Z"},"trusted":true},"outputs":[],"source":["import os\n","import shutil\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.applications import EfficientNetB0, ResNet50\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","import numpy as np\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score\n","import seaborn as sns\n","import pandas as pd"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T16:00:39.749228Z","iopub.status.busy":"2024-05-30T16:00:39.748417Z","iopub.status.idle":"2024-05-30T16:00:39.769953Z","shell.execute_reply":"2024-05-30T16:00:39.768508Z","shell.execute_reply.started":"2024-05-30T16:00:39.749193Z"},"trusted":true},"outputs":[],"source":["# Define dataset directories\n","dataset_dir = 'ishafolk/folk-art-dataset-main'\n","\n","\n","# Create directories for train, validation, and test splits\n","base_dir = 'folk-art-data'\n","train_dir = os.path.join(base_dir, 'train')\n","val_dir = os.path.join(base_dir, 'validation')\n","test_dir = os.path.join(base_dir, 'test')\n","\n","os.makedirs(train_dir, exist_ok=True)\n","os.makedirs(val_dir, exist_ok=True)\n","os.makedirs(test_dir, exist_ok=True)\n","\n","# Get class names\n","\n","class_names = [c for c in sorted(os.listdir(dataset_dir)) if c != \"README.md\"]\n","\n","for c in class_names:\n","    os.makedirs(os.path.join(train_dir, c), exist_ok=True)\n","    os.makedirs(os.path.join(val_dir, c), exist_ok=True)\n","    os.makedirs(os.path.join(test_dir, c), exist_ok=True)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T16:00:46.588053Z","iopub.status.busy":"2024-05-30T16:00:46.587691Z","iopub.status.idle":"2024-05-30T16:02:33.463021Z","shell.execute_reply":"2024-05-30T16:02:33.461622Z","shell.execute_reply.started":"2024-05-30T16:00:46.588027Z"},"trusted":true},"outputs":[],"source":["\n","def split_data(source_dir, train_dir, val_dir, test_dir, split_ratio=(0.7, 0.15, 0.15)):\n","    for category in os.listdir(source_dir):\n","        category_path = os.path.join(source_dir, category)\n","        if not os.path.isdir(category_path):  # Check if it's a directory\n","            continue\n","        images = os.listdir(category_path)\n","        train, temp = train_test_split(images, test_size=(1 - split_ratio[0]), random_state=42)\n","        val, test = train_test_split(temp, test_size=0.5, random_state=42)\n","\n","        for img in train:\n","            shutil.copy(os.path.join(category_path, img), os.path.join(train_dir, category))\n","        for img in val:\n","            shutil.copy(os.path.join(category_path, img), os.path.join(val_dir, category))\n","        for img in test:\n","            shutil.copy(os.path.join(category_path, img), os.path.join(test_dir, category))\n","\n","# Split the dataset\n","split_data(dataset_dir, train_dir, val_dir, test_dir)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T16:02:37.672662Z","iopub.status.busy":"2024-05-30T16:02:37.672287Z","iopub.status.idle":"2024-05-30T16:02:37.678379Z","shell.execute_reply":"2024-05-30T16:02:37.677083Z","shell.execute_reply.started":"2024-05-30T16:02:37.672635Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Class names: ['Aipan Art (Uttarakhand)', 'Assamese Miniature Painting (Assam)', 'Basholi Painting (Jammu and Kashmir)', 'Bhil Painting (Madhya Pradesh)', 'Chamba Rumal (Himachal Pradesh)', 'Cheriyal Scroll Painting (Telangana)', 'Dokra Art(West Bengal)', 'Gond Painting (Madhya Pradesh)', 'Kalamkari Painting (Andra Pradesh and Telangana)', 'Kalighat Painting (West Bengal)', 'Kangra Painting (Himachal Pradesh)', 'Kerala Mural Painting (Kerala)', 'Kondapalli Bommallu (Andra Pradesh)', 'Kutch Lippan Art (Gujarat)', 'Leather Puppet Art (Andra Pradesh)', 'Madhubani Painting (Bihar)', 'Mandala Art', 'Mandana Art (Rajasthan)', 'Mata Ni Pachedi (Gujarat)', 'Meenakari Painting (Rajasthan)', 'Mughal Paintings', 'Mysore Ganjifa Art (Karnataka)', 'Pattachitra Painting (Odisha and Bengal)', 'Patua Painting (West Bengal)', 'Pichwai Painting (Rajasthan)', 'Rajasthani Miniature Painting (Rajasthan)', 'Rogan Art from Kutch (Gujarat)', 'Sohrai Art (Jharkhand)', 'Tikuli Art (Bihar)', 'Warli Folk Painting (Maharashtra)']\n"]}],"source":["print(\"Class names:\", class_names)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T12:54:32.405281Z","iopub.status.busy":"2024-05-30T12:54:32.404883Z","iopub.status.idle":"2024-05-30T14:13:38.896194Z","shell.execute_reply":"2024-05-30T14:13:38.895198Z","shell.execute_reply.started":"2024-05-30T12:54:32.405251Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 22601 images belonging to 30 classes.\n","Found 4845 images belonging to 30 classes.\n","Found 4864 images belonging to 30 classes.\n","Epoch 1/50\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m  1/707\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18:23:47\u001b[0m 94s/step - accuracy: 0.0312 - loss: 3.5219"]},{"name":"stderr","output_type":"stream","text":["WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","I0000 00:00:1717073770.004489     169 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m157/707\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:17\u001b[0m 359ms/step - accuracy: 0.1968 - loss: 3.0453"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m707/707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m419s\u001b[0m 461ms/step - accuracy: 0.4363 - loss: 2.1312 - val_accuracy: 0.5534 - val_loss: 1.6079\n","Epoch 2/50\n","\u001b[1m707/707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 378ms/step - accuracy: 0.7564 - loss: 0.8341 - val_accuracy: 0.7670 - val_loss: 0.8210\n","Epoch 3/50\n","\u001b[1m707/707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 374ms/step - accuracy: 0.8166 - loss: 0.6182 - val_accuracy: 0.7959 - val_loss: 0.7238\n","Epoch 4/50\n","\u001b[1m707/707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 372ms/step - accuracy: 0.8632 - loss: 0.4613 - val_accuracy: 0.8091 - val_loss: 0.6962\n","Epoch 5/50\n","\u001b[1m707/707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 383ms/step - accuracy: 0.8882 - loss: 0.3714 - val_accuracy: 0.8194 - val_loss: 0.6475\n","Epoch 6/50\n","\u001b[1m707/707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 376ms/step - accuracy: 0.9054 - loss: 0.3084 - val_accuracy: 0.8332 - val_loss: 0.6597\n","Epoch 7/50\n","\u001b[1m707/707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 373ms/step - accuracy: 0.9219 - loss: 0.2519 - val_accuracy: 0.8388 - val_loss: 0.6357\n","Epoch 8/50\n","\u001b[1m707/707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 374ms/step - accuracy: 0.9330 - loss: 0.2157 - val_accuracy: 0.8440 - val_loss: 0.6750\n","Epoch 9/50\n","\u001b[1m707/707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 372ms/step - accuracy: 0.9414 - loss: 0.1804 - val_accuracy: 0.8417 - val_loss: 0.7034\n","Epoch 10/50\n","\u001b[1m707/707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 373ms/step - accuracy: 0.9496 - loss: 0.1602 - val_accuracy: 0.8477 - val_loss: 0.7050\n","Epoch 11/50\n","\u001b[1m707/707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 372ms/step - accuracy: 0.9536 - loss: 0.1476 - val_accuracy: 0.8442 - val_loss: 0.7007\n","Epoch 12/50\n","\u001b[1m707/707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 371ms/step - accuracy: 0.9570 - loss: 0.1325 - val_accuracy: 0.8456 - val_loss: 0.7171\n","Epoch 13/50\n","\u001b[1m707/707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 373ms/step - accuracy: 0.9610 - loss: 0.1235 - val_accuracy: 0.8493 - val_loss: 0.7491\n","Epoch 14/50\n","\u001b[1m707/707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 373ms/step - accuracy: 0.9668 - loss: 0.1114 - val_accuracy: 0.8464 - val_loss: 0.7673\n","Epoch 15/50\n","\u001b[1m707/707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 371ms/step - accuracy: 0.9662 - loss: 0.1042 - val_accuracy: 0.8563 - val_loss: 0.7249\n","Epoch 16/50\n","\u001b[1m707/707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 369ms/step - accuracy: 0.9679 - loss: 0.1041 - val_accuracy: 0.8520 - val_loss: 0.7590\n","Epoch 17/50\n","\u001b[1m707/707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 369ms/step - accuracy: 0.9669 - loss: 0.0953 - val_accuracy: 0.8559 - val_loss: 0.7484\n"]}],"source":["# Data augmentation and generators\n","train_datagen = ImageDataGenerator(\n","    rescale=1./255,\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")\n","\n","val_test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","train_generator = train_datagen.flow_from_directory(\n","    train_dir,\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='categorical',\n","    classes=class_names  # Use the actual class names, excluding \"readme\"\n",")\n","\n","val_generator = val_test_datagen.flow_from_directory(\n","    val_dir,\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='categorical',\n","    classes=class_names  # Use the actual class names, excluding \"readme\"\n",")\n","\n","test_generator = val_test_datagen.flow_from_directory(\n","    test_dir,\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='categorical',\n","    classes=class_names  # Use the actual class names, excluding \"readme\"\n",")\n","\n","\n","# Build and compile EfficientNetB0 model\n","base_model_efficientnet = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n","x = base_model_efficientnet.output\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(1024, activation='relu')(x)\n","predictions = Dense(30, activation='softmax')(x)  # Output layer with 30 units\n","\n","model_efficientnet = Model(inputs=base_model_efficientnet.input, outputs=predictions)\n","model_efficientnet.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Callbacks\n","checkpoint_efficientnet = ModelCheckpoint('efficientnet_best_model.weights.h5', monitor='val_accuracy', save_best_only=True, mode='max', save_weights_only=True)\n","early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","\n","# Train EfficientNet model\n","history_efficientnet = model_efficientnet.fit(\n","    train_generator,\n","    epochs=50,\n","    validation_data=val_generator,\n","    callbacks=[checkpoint_efficientnet, early_stopping]\n",")"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T14:20:33.463358Z","iopub.status.busy":"2024-05-30T14:20:33.462408Z","iopub.status.idle":"2024-05-30T15:06:43.048136Z","shell.execute_reply":"2024-05-30T15:06:43.047154Z","shell.execute_reply.started":"2024-05-30T14:20:33.463322Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n","\u001b[1m707/707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 421ms/step - accuracy: 0.5133 - loss: 1.7566 - val_accuracy: 0.2698 - val_loss: 2.9835\n","Epoch 2/50\n","\u001b[1m707/707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 373ms/step - accuracy: 0.7742 - loss: 0.7681 - val_accuracy: 0.7459 - val_loss: 0.9118\n","Epoch 3/50\n","\u001b[1m707/707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 371ms/step - accuracy: 0.8300 - loss: 0.5643 - val_accuracy: 0.7269 - val_loss: 1.0667\n","Epoch 4/50\n","\u001b[1m707/707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 374ms/step - accuracy: 0.8669 - loss: 0.4430 - val_accuracy: 0.7595 - val_loss: 0.9867\n","Epoch 5/50\n","\u001b[1m707/707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 376ms/step - accuracy: 0.8867 - loss: 0.3649 - val_accuracy: 0.7860 - val_loss: 0.8653\n","Epoch 6/50\n","\u001b[1m707/707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 372ms/step - accuracy: 0.9057 - loss: 0.3029 - val_accuracy: 0.7853 - val_loss: 0.9133\n","Epoch 7/50\n","\u001b[1m707/707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 376ms/step - accuracy: 0.9117 - loss: 0.2725 - val_accuracy: 0.8058 - val_loss: 0.8373\n","Epoch 8/50\n","\u001b[1m707/707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 375ms/step - accuracy: 0.9225 - loss: 0.2468 - val_accuracy: 0.8252 - val_loss: 0.8187\n","Epoch 9/50\n","\u001b[1m707/707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 372ms/step - accuracy: 0.9227 - loss: 0.2385 - val_accuracy: 0.8083 - val_loss: 0.9637\n","Epoch 10/50\n","\u001b[1m707/707\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 372ms/step - accuracy: 0.9351 - loss: 0.2070 - val_accuracy: 0.8045 - val_loss: 0.8999\n"]}],"source":["# Build and compile ResNet50 model\n","base_model_resnet = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n","x = base_model_resnet.output\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(1024, activation='relu')(x)\n","predictions = Dense(30, activation='softmax')(x)\n","\n","model_resnet = Model(inputs=base_model_resnet.input, outputs=predictions)\n","model_resnet.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Callbacks for ResNet\n","# Callbacks for ResNet\n","checkpoint_resnet = ModelCheckpoint('resnet_best_model.weights.h5', monitor='val_accuracy', save_best_only=True, mode='max', save_weights_only=True)\n","# Train ResNet model\n","history_resnet = model_resnet.fit(\n","    train_generator,\n","    epochs=50,\n","    validation_data=val_generator,\n","    callbacks=[checkpoint_resnet, early_stopping]\n",")\n"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T15:22:07.263583Z","iopub.status.busy":"2024-05-30T15:22:07.262887Z","iopub.status.idle":"2024-05-30T15:22:28.464552Z","shell.execute_reply":"2024-05-30T15:22:28.463526Z","shell.execute_reply.started":"2024-05-30T15:22:07.263548Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - accuracy: 0.8705 - loss: 0.6269\n","EfficientNet Test Accuracy: 86.31%\n","\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 62ms/step - accuracy: 0.8315 - loss: 0.7460\n","ResNet Test Accuracy: 82.73%\n"]}],"source":["# Load best models and evaluate\n","model_efficientnet.load_weights('efficientnet_best_model.weights.h5')\n","model_resnet.load_weights('resnet_best_model.weights.h5')\n","\n","efficientnet_eval = model_efficientnet.evaluate(test_generator)\n","print(f'EfficientNet Test Accuracy: {efficientnet_eval[1]*100:.2f}%')\n","\n","resnet_eval = model_resnet.evaluate(test_generator)\n","print(f'ResNet Test Accuracy: {resnet_eval[1]*100:.2f}%')\n"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T15:41:08.053711Z","iopub.status.busy":"2024-05-30T15:41:08.053329Z","iopub.status.idle":"2024-05-30T15:41:08.642636Z","shell.execute_reply":"2024-05-30T15:41:08.641833Z","shell.execute_reply.started":"2024-05-30T15:41:08.053681Z"},"trusted":true},"outputs":[],"source":["model_efficientnet.save('efficientnet_model.h5')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5113610,"sourceId":8556444,"sourceType":"datasetVersion"},{"datasetId":5114266,"sourceId":8557280,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":4}
